\chapter{Methodology}

The methods used to address the research topics raised in this thesis are described in this chapter. To obtain the results that will be presented, this section provides a thorough overview of the pipeline (Figure~\ref{fig:pipeline}), outlining the procedures for preprocessing data, extracting features, model selection and training, and assessing performance. Through the conversion of theoretical ideas into practical procedures, aiming to offer an outline of how these methods are applied in real-life situations.

\fig{img/pipeline}{Implementation pipeline}{fig:pipeline}{0.65}

\section{Data Collection}

The collection of data is a crucial step that lays the foundation for later analysis and model development. It involves gathering textual data from various sources, which could include websites, databases, or specialized datasets for the specific domain of interest. It is important to collect a sufficiently diverse amount of data to capture the variability present in real-world text data \citep{openai_gpt3}. This thesis uses a BBC news dataset containing 2225 text data and five categories of documents \citep{text_dataset}. 

\section{Data and Text preprocessing}

Preprocessing methods are an essential step for text mining techniques and applications. 
The data and its columns need to be analyzed and inspected since it is often necessary to generate a new column combining the various features. Through the joint column, a better, more comprehensive, and more accurate analysis can take place. The three essential preprocessing steps — tokenization, lowercase, stop word removal, and TF/IDF algorithms — are covered in this study (Figure 1).

\subsection{Tokenization}

Breaking phrases up into individual words, characters, and punctuation — so called tokens — is a process known as tokenization \citep{tabassum_survey_2020}. The split function uses white spaces or punctuations as dividing criteria. These generated tokens are often stored in a list afterward. In later processing phases, this step aids in removing unnecessary terms \citep{tabassum_survey_2020}. 

For example:

“This is an example sentence for the showcase of tokenization!”

Will be split into:

“This”, “is”, “an”, “example”, “sentence”, “for”, “the”, “showcase”, “of”, “tokenization”, “!”

\subsection{Lowercase Conversion}

Text usually uses abbreviations and uppercase characters. Despite being often overlooked, this text preprocessing step is one of the simplest and most effective \citep{tabassum_survey_2020}. NLP is case-sensitive, meaning, it interprets 'Hello' differently than 'hello' and leads to a different outcome. In the later phases of word embedding would it create two distinct vectors, for the same words with one in capital and one in lowercase. For this reason, the best practice in text pre-processing has been to make all words lowercase \citep{tabassum_survey_2020}.

\subsection{StopWords Removal}

Simple words like "the", "are", "is", "and" and so forth have no significance except in certain particular use cases. For instance, these extra words are not given any weightage in the text classification use case \citep{tabassum_survey_2020}. The only keywords that are extracted are those that define the themes. Thus, these StopWords need to be located and eliminated from their content in order for the algorithms to produce the optimum results. It is important to bear in mind that certain circumstances, including conversational models, require the use of certain negation terms, such as "No," "cannot," "wont," and "not" \citep{tabassum_survey_2020}. Libraries like NLTK and sklearn already offer predefined lists of StopWords which can be easily downloaded and implemented into the code.

\section{Feature Extraction}

Feature extraction involves converting features into vector representations for machine comprehension tasks. Following extraction, each feature is transformed into a vector format before being fed into classifier models. Common approaches, such as \ac{bow} and \ac{tfidf}, are widely used for this purpose and will be investigated further.

\subsection{Bag-of-Words (BoW)}

In natural language processing, a Bag-of-Words is a list of words ranked by how frequently they appear in a certain text or document. It only counts the frequency of a word, regardless of its position in the text \citep{tabassum_survey_2020}. Thus, the BoW interprets that documents or texts containing similar words share the same context. One flaw of the model is that it prioritizes words that appear more frequently, making them more significant. On the other hand, some words may occur more frequently than others but lack sufficient information to help in clustering or classification issues. Additionally, longer documents provide a greater rate than shorter ones, which reduces the accuracy of the BoW model \citep{tabassum_survey_2020}. Example text \citep{openai_gpt3}:

\begin{verbatim}
Document 0: "The quick brown fox"
Document 1: "Jumped over the lazy dog"
Document 2: "The dog chased the fox"

Vocabulary: 
{'the': 8, 'quick': 7, 'brown': 0, 'fox': 3, 'jumped': 4,
'over': 6, 'lazy': 5, 'dog': 2, 'chased': 1}
BoW:
    brown  chased  dog  fox  jumped  lazy  over  quick  the
0      1      0     0    1      0     0     0      1     1
1      0      0     1    0      1     1     1      0     1
2      0      1     1    1      0     0     0      0     2
\end{verbatim}


\subsection{Term Frequency-Inverse Document Frequency (TF-IDF)}

\ac{tfidf} is a popular technique in text mining and information retrieval that weights words according to how frequently they appear in a document compared to how frequently they occur throughout the corpus \citep{vijayarani_preprocessing_2015}. This weighting mechanism effectively mitigates the influence of commonly used words. Employing TF-IDF for StopWords filtering proves beneficial across various domains, including text classification and summarization. The model combines two key metrics: term frequency and inverse document frequency, wherein the former tallies the occurrences of each term in documents, while the latter adjusts for the term's prevalence across the corpus \citep{vijayarani_preprocessing_2015}. By taking the logarithm of the ratio of all documents to the instances of a term within a document, TF-IDF effectively diminishes the importance of less significant words \citep{tabassum_survey_2020}. Example text \citep{openai_gpt3}: 

\begin{verbatim}
Document 0: "The quick brown fox"
Document 1: "Jumped over the lazy dog"
Document 2: "The dog chased the fox"

Vocabulary: 
{'the': 8, 'quick': 7, 'brown': 0, 'fox': 3, 'jumped': 4,
'over': 6, 'lazy': 5, 'dog': 2, 'chased': 1}

TF-IDF:
  brown  chased  dog  fox   jumped  lazy   over  quick   the 
0  0.58   0.00  0.00  0.44   0.00   0.00   0.00   0.58   0.35  
1  0.00   0.00  0.38  0.00   0.50   0.50   0.50   0.00   0.30  
2  0.00   0.53  0.40  0.40   0.00   0.00   0.00   0.00   0.63   
\end{verbatim}

\section{Train-Test-Split}

Train-Test-Split is a common method provided by sklearn, used to divide a given dataset into smaller subsets. To evaluate the models in a fair manner, the datasets must be split into training and testing sets. By dividing the data, the model's performance on unknown data may be evaluated, revealing whether the model overfits or performs well in terms of generalization. This stage is crucial because the model's ultimate objective is to accurately predict new data.

To keep the comparison as equitable and reproducible as possible, each dataset was split just once, and the same split was used for training and fine-tuning all models. Listing~\ref{lst:train-test-split} displays the implementation, which divides the data into a split of 80:20, where the training set contains 80\%, and the testing set 20\%. 

\begin{lstlisting}[language=Python, caption=Train-Test-Split in Python \citep{sklearn_ttt}, label=lst:train-test-split]
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
\end{lstlisting}

The variable X (features) is assigned to the previously created vector and y (target) to the labels that should be predicted. 

The random\_state parameter controls the randomness of the data splitting process. Reproducibility is ensured by the data splitting procedure, which yields the same result each time it is run when a specified random\_state value is supplied. If random\_state is not specified, the data splitting will be different each time the function is called.

\section{Model Selection}

Supervised, unsupervised, and reinforcement learning are the three primary subcategories of machine learning. This thesis only required supervised learning, which is about the prediction of values with regression models, as well as classifying data with predefined labels. On the other hand, there is unsupervised learning which contains the analysis of patterns and can form clusters out of unlabelled data.

\subsection{Classification Models}

There are several different classification models and each of them fits a specific use case best. 
The models need to be evaluated and compared to one and another, to find the optimal algorithm. This study analyses seven different models from sklearn (Naive Bayes, Random Forest, Support Vector Machine, KNeighborsClassifier, Logistic Regression, Decision Tree, XGBoost) and evaluates them based on the run time and accuracy. 

\subsubsection{K-nearest neighbour - KNN}
One of the best instances of instance-based learning is the \ac{knn} algorithm \citep{sen_supervised_2020}. Additionally, it is easy to understand and a simple method for classification problems. Despite its simplicity, it has the capability to yield results that are highly competitive. Not only is it well suitable for classifications but it also fits the requirements for regression predictions \citep{sen_supervised_2020}.

The algorithm stores all the given data points and predicts the target based on giving attention to the similarity measurements of the surrounding neighbours in likelihood \citep{sen_supervised_2020}. The number of neighbours that will be taken in consideration is defined by the "k" variable. Assuming k equals 3, a circular region with the new data point as its centroid is created to encompass only the three closest neighbouring data points on the plane. The determination of the label for the new data point is then based on the distances between the data point and each of its neighbours \citep{sen_supervised_2020}.

Some of the advantages are that it handles noisy and large training data well, besides the simplicity of the implementation \citep{sen_supervised_2020}. A significant limitation of this algorithm arises from the necessity to recalculate the distances from K neighbours for every new instance, resulting in substantial computational time consumption. Additionally, accurately determining the value of K is crucial to achieve a lower error rate \citep{sen_supervised_2020}.

\subsubsection{Support Vector Machine - SVM}
Another supervised algorithm is the \ac{svm}. It can handle both, regression, and classification problems, though is it more seen for classification. Furthermore, it can manage numerous instances that involve both continuous and categorical data \citep{sen_supervised_2020}.

The algorithm can be defined like following. Items of the dataset with "n" features will be characterised and plotted as points in an n-dimensional space split into classes by a hyperplane with the widest possible margin. The data points are then mapped into the previous defined space to predict their label based on their position relative to the hyperplane \citep{sen_supervised_2020}.

A significant performance boost can be seen, when the variable "n" exceeds the total size of sample set. Therefore, is this algorithm mostly taken under consideration for high-dimensional data \citep{sen_supervised_2020}. Further improvements in performance can be achieved by having a well-constructed hyperplane. Despite its advantages, is a relatively high training time one of its drawbacks. Which leads to slower predictions, especially with large datasets \citep{sen_supervised_2020}.

\subsubsection{Decision Tree - DT}

One kind of supervised learning method for regression and classification is the use of \acfp{dt} \citep{sklearn_dtt}. The goal is to build a model that can use basic decision rules inferred from the features of the data to forecast the value of a target variable. Consider a piecewise constant approximation as a tree \citep{sklearn_dtt}. For instance, decision trees estimate a sine curve depending on inputs by combining a set of if-then-else decision rules. As the tree goes deeper, the model fits the data better and the decision criteria get increasingly complex \citep{sklearn_dtt}.

The ease of use and interpretability of decision trees is one of its main benefits. It is possible to visualise and comprehend the tree structure, so even non-experts may use it \citep{sklearn_dtt}. Furthermore, because decision trees can handle both numerical and categorical data without requiring a lot of preprocessing, they also require less preparation of the data. They also have the benefit of handling multi-output issues and offering a white box approach, in which a decision's logic may be simply described using boolean logic \citep{sklearn_dtt}.

Nevertheless, decision trees can overfit, especially if they get too complicated. To avoid this problem, measures like trimming and imposing tree growth restrictions are required \citep{sklearn_dtt}. Decision trees can also be unstable since slight changes in the data might produce noticeably different tree architectures. Despite these drawbacks, is it a useful tool in machine learning, even with these limitations, especially where simplicity and interpretability are top priorities \citep{sklearn_dtt}.


\subsection{Deep Learning}
\ac{dl} is a specific category within \ac{ml} methodologies that utilizes Artificial Neural Networks (ANN). The structure of neurons in the human brain serves as a loose inspiration for these networks \citep{gulli_deep_2017}. Informally, the term "deep" originally referred to the presence of numerous layers in the artificial neural network. However, this definition has evolved over time. While just four years ago, having 10 layers was considered sufficient to qualify a network as deep, today it is more commonplace to characterize a network as deep when it comprises hundreds of layers \citep{gulli_deep_2017}. 

Keras serves as a user-friendly high-level deep learning library in Python, providing a convenient interface for building neural networks \citep{gulli_deep_2017}. The sequential model in Keras is a linear stack of layers, allowing the straightforward construction of neural networks by sequentially adding layers. Each layer adds non-linearity to the model, allowing it to capture intricate patterns and correlations within the data, frequently using activation functions like Rectified Linear Unit (ReLU) \citep{gulli_deep_2017}. 

