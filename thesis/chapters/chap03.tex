\chapter{Methodology}

The methods used to address the research topics raised in this thesis are described in this chapter. To obtain the results that will be presented, this chapter aims to give a thorough description of the data collection and preprocessing, feature extraction, and model selection. To ensure reproducibility and provide the reader with a fair basis for evaluating the employment of models on text categorization tasks, this section should detail the techniques.

\section{Data Collection}

The collection of data is a crucial step that lays the foundation for later analyses and model development. It involves gathering textual data from various sources, which could include websites, databases, or specialized datasets for the specific domain of interest. It is important to collect a sufficiently diverse amount of data to capture the variability present in real-world text data (OpenAI, 2024). This thesis uses a BBC news dataset containing 2225 text data and five categories of documents (Text Document Classification Dataset, 2024). 

\section{Text preprocessing}

Preprocessing methods are an essential step for text mining techniques and applications. The data and its columns need to be analyzed and inspected since it is often necessary to generate a new column combining the various features. Through the joint column, a better, more comprehensive, and more accurate analysis can take place. The three essential preprocessing steps — extraction, lowercase conversion, and StopWords removal — are covered in this study (Figure 1).

\subsection{Extraction / Tokenization}

Tokenization is the process of splitting sentences into individual words, characters, and punctuation, which are referred to as tokens. The split function uses white spaces or punctuations as dividing criteria. These generated tokens are often stored in a list afterward. In later processing phases, this step aids in removing unnecessary terms (Tabassum \& Patil, 2020). 

For example:

“This is an example sentence for the showcase of tokenization!”

Will be split into:

“This”, “is”, “an”, “example”, “sentence”, “for”, “the”, “showcase”, “of”, “tokenization”, “!”

\subsection{Lowercase Conversion}

Text typically consists of capital letters and abbreviations. Although this stage of text preprocessing is frequently skipped, it is one of the easiest and most successful ones. NLP is case-sensitive, meaning, it interprets 'Hello' differently than 'hello' and leads to a different outcome. In the later phases of word embedding would it create two distinct vectors, for the same words with one in capital and one in lowercase. For this reason, the best practice in text pre-processing has been to make all words lowercase (Tabassum \& Patil, 2020).

\subsection{StopWords Removal}

Simple words like "the", "are", "is", "and" and so forth have no significance except in certain particular use cases. For instance, these extra words are not given any weightage in the text classification use case. The keywords that define the topics are the only ones that are extracted. Therefore, to reach the best result of algorithms these StopWords have to be found and removed from their document. It is also important to remember that in some scenarios, such as conversational models, the inclusion of specific negation words, like “No”, “cannot”, “wont” and “not”, is crucial (Tabassum \& Patil, 2020). Libraries like nltk and sklearn already offer predefined lists of StopWords which can be easily downloaded and implemented into the code.

\section{Feature Extraction}

The encoding of features in vector forms for machine comprehension is often referred to as feature extraction. After being extracted by these methods, every feature is finally represented as a vector, which is then sent to the classifier models. The most common techniques, such as Bag-of-Words (BoW), and TF-IDF will be discussed next.

\subsection{Bag-of-Words (BoW)}

A bag of words in terms of natural language processing is a collection of words based on the number of occurrences in a given text or document. It only counts the frequency of a word, regardless of its position in the text. Thus, the BoW interprets that documents or texts containing similar words share the same context. One flaw of the model is that it prioritizes words that appear more frequently, making them more significant. On the other hand, some words may occur more frequently than others but lack sufficient information to help in clustering or classification issues. Additionally, longer documents provide a greater rate than shorter ones, which reduces the accuracy of the BoW model (Tabassum \& Patil, 2020).

\begin{verbatim}
Document 0: "The quick brown fox"
Document 1: "Jumped over the lazy dog"
Document 2: "The dog chased the fox"

Vocabulary: 
{'the': 8, 'quick': 7, 'brown': 0, 'fox': 3, 'jumped': 4,
'over': 6, 'lazy': 5, 'dog': 2, 'chased': 1}

BoW:
    brown  chased  dog  fox  jumped  lazy  over  quick  the
0      1      0     0    1      0     0     0      1     1
1      0      0     1    0      1     1     1      0     1
2      0      1     1    1      0     0     0      0     2
\end{verbatim}


\subsection{Term Frequency-Inverse Document Frequency (TF-IDF)}

A numerical measure called Term Frequency-Inverse Document Frequency (tf-idf) shows how significant a word is to a document within a collection. It is mostly used as a common weighting factor in text mining and information retrieval. The value of tf-idf rises in direct proportion to the frequency of a word in the corpus, but this is offset by the term's frequency in the document. This can help in managing the fact that certain words are typically used more commonly than others. Stop-word filtering using Tf-IDF is effective in a variety of subject areas, such as text classification and summarization. The model is the product of the two aforementioned statistics, termed frequency and inverse document frequency. The number of occurrences with which each term appears in each document is counted and added together to further differentiate them (Vijayarani et al., 2015). By calculating the log of the ratio of all documents to all instances of a word in a given document, it essentially scales down the less important words (Tabassum \& Patil, 2020).

\begin{verbatim}
Document 0: "The quick brown fox"
Document 1: "Jumped over the lazy dog"
Document 2: "The dog chased the fox"

Vocabulary: 
{'the': 8, 'quick': 7, 'brown': 0, 'fox': 3, 'jumped': 4,
'over': 6, 'lazy': 5, 'dog': 2, 'chased': 1}

TF-IDF:
  brown  chased  dog  fox   jumped  lazy   over  quick   the 
0  0.58   0.00  0.00  0.44   0.00   0.00   0.00   0.58   0.35  
1  0.00   0.00  0.38  0.00   0.50   0.50   0.50   0.00   0.30  
2  0.00   0.53  0.40  0.40   0.00   0.00   0.00   0.00   0.63   
\end{verbatim}

\section{Model Selection}

xx